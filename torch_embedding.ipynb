{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3749, -0.5688, -0.4704],\n",
      "         [ 0.4444, -0.7900, -0.8251],\n",
      "         [ 1.4089, -0.7410, -0.8014],\n",
      "         [ 0.7591, -0.3077, -1.0916]],\n",
      "\n",
      "        [[ 1.4089, -0.7410, -0.8014],\n",
      "         [ 0.7296, -0.4963, -0.9164],\n",
      "         [ 0.4444, -0.7900, -0.8251],\n",
      "         [ 0.4487, -1.5731,  0.6695]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473d025f9f524d5686879279e78a7f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Gemma2Model, Gemma2Config\n",
    "\n",
    "config = Gemma2Config()\n",
    "\n",
    "model = Gemma2Model.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-25): 26 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2Model(\n",
      "  (embed_tokens): Embedding(256, 8, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Gemma2DecoderLayer(\n",
      "      (self_attn): Gemma2Attention(\n",
      "        (q_proj): Linear(in_features=8, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=8, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=8, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=8, bias=False)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Gemma2MLP(\n",
      "        (gate_proj): Linear(in_features=8, out_features=9216, bias=False)\n",
      "        (up_proj): Linear(in_features=8, out_features=9216, bias=False)\n",
      "        (down_proj): Linear(in_features=9216, out_features=8, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma2RMSNorm()\n",
      "      (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
      "      (post_feedforward_layernorm): Gemma2RMSNorm()\n",
      "      (post_attention_layernorm): Gemma2RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma2RMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Gemma2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Placeholder for rotary embedding initialization\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply rotary embedding\n",
    "        return x\n",
    "\n",
    "\n",
    "class PytorchGELUTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x) * torch.tanh(x)\n",
    "\n",
    "\n",
    "class Gemma2RMSNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.norm(2, dim=-1, keepdim=True)\n",
    "        return x * self.weight / (norm + self.eps)\n",
    "\n",
    "\n",
    "class Gemma2Attention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(embed_dim, 2048, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, 1024, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, 1024, bias=False)\n",
    "        self.o_proj = nn.Linear(2048, embed_dim, bias=False)\n",
    "        self.rotary_emb = Gemma2RotaryEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        q, k = self.rotary_emb(q), self.rotary_emb(k)\n",
    "        # Implement attention mechanism\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / q.size(-1)**0.5\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        return self.o_proj(context)\n",
    "\n",
    "\n",
    "class Gemma2MLP(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(embed_dim, 9216, bias=False)\n",
    "        self.up_proj = nn.Linear(embed_dim, 9216, bias=False)\n",
    "        self.down_proj = nn.Linear(9216, embed_dim, bias=False)\n",
    "        self.act_fn = PytorchGELUTanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x) * self.up_proj(x)))\n",
    "\n",
    "\n",
    "class Gemma2DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = Gemma2Attention(embed_dim)\n",
    "        self.mlp = Gemma2MLP(embed_dim)\n",
    "        self.input_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attn(self.input_layernorm(x))\n",
    "        x = x + self.mlp(self.pre_feedforward_layernorm(x))\n",
    "        return self.post_feedforward_layernorm(x)\n",
    "\n",
    "\n",
    "class Gemma2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.layers = nn.ModuleList([Gemma2DecoderLayer(embed_dim) for _ in range(num_layers)])\n",
    "        self.norm = Gemma2RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Example instantiation\n",
    "model = Gemma2Model(vocab_size=256, embed_dim=8, num_layers=4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2Model(\n",
      "  (embed_tokens): Embedding(256, 8, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Gemma2DecoderLayer(\n",
      "      (self_attn): Gemma2Attention(\n",
      "        (q_proj): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (k_proj): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (v_proj): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (o_proj): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Gemma2MLP(\n",
      "        (gate_proj): Linear(in_features=8, out_features=16, bias=False)\n",
      "        (up_proj): Linear(in_features=8, out_features=16, bias=False)\n",
      "        (down_proj): Linear(in_features=16, out_features=8, bias=False)\n",
      "        (act_fn): GELU(approximate='none')\n",
      "      )\n",
      "      (input_layernorm): Gemma2RMSNorm()\n",
      "      (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
      "      (post_feedforward_layernorm): Gemma2RMSNorm()\n",
      "      (post_attention_layernorm): Gemma2RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma2RMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Gemma2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * x\n",
    "\n",
    "class Gemma2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        return cos, sin\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class Gemma2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, (self.num_heads//2) * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, (self.num_heads//2) * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.rotary_emb = Gemma2RotaryEmbedding(self.head_dim)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "        \n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        q = q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads//2, self.head_dim)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads//2, self.head_dim)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_length)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        k = k.repeat(1, 1, 2, 1)  # Repeat to match number of heads\n",
    "        v = v.repeat(1, 1, 2, 1)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "            \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "class Gemma2MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate = self.act_fn(self.gate_proj(x))\n",
    "        up = self.up_proj(x)\n",
    "        return self.down_proj(gate * up)\n",
    "\n",
    "class Gemma2DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = Gemma2Attention(config)\n",
    "        self.mlp = Gemma2MLP(config)\n",
    "        \n",
    "        self.input_layernorm = Gemma2RMSNorm(config.hidden_size)\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size)\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Self attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states, attention_mask)\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        # MLP\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.pre_feedforward_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = self.post_feedforward_layernorm(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "class Gemma2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.layers = nn.ModuleList([Gemma2DecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = Gemma2RMSNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "            \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "config = Gemma2Config(\n",
    "    vocab_size=256,\n",
    "    hidden_size=8,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=2,\n",
    "    head_dim=4,\n",
    "    intermediate_size=16\n",
    ")\n",
    "\n",
    "model = Gemma2Model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(256, 8, padding_idx=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens = nn.Embedding(256, 8, padding_idx=0)\n",
    "embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7893, -1.2624,  1.7711,  0.6604,  1.3104, -1.6788, -0.0572, -0.6773],\n",
       "        [-0.3582, -0.4415, -0.0198, -0.9026,  0.8654, -2.0028, -0.7236, -0.2078],\n",
       "        [-0.1072,  1.5299,  0.3908, -0.2816, -0.1998, -0.7492, -1.3824,  1.2076]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = [1, 5, 3]\n",
    "embed_tokens(torch.tensor(example_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def precompute_rotary_embedding(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(seq_len)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "\n",
    "def apply_rotary_embedding(x, freqs):\n",
    "    \"\"\"\n",
    "    Apply rotary embedding to the input tensor.\n",
    "    x: Tensor of shape [batch_size, seq_len, num_heads, head_dim].\n",
    "    freqs: Tensor of shape [seq_len, head_dim // 2].\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, num_heads, head_dim = x.shape\n",
    "\n",
    "    # Ensure head_dim is divisible by 2\n",
    "    assert head_dim % 2 == 0, \"head_dim must be divisible by 2 for rotary embeddings\"\n",
    "\n",
    "    # Adjust freqs to match the current sequence length\n",
    "    freqs = freqs[:seq_len].to(x.device)  # Slice freqs to match seq_len\n",
    "\n",
    "    # Split head_dim into real and imaginary parts\n",
    "    x = x.view(batch_size, seq_len, num_heads, head_dim // 2, 2)\n",
    "    x_complex = torch.view_as_complex(x)  # Convert to complex numbers\n",
    "\n",
    "    # Expand freqs to match [seq_len, num_heads, head_dim // 2]\n",
    "    freqs = freqs.unsqueeze(1).repeat(1, num_heads, 1)\n",
    "\n",
    "    # Apply rotary embedding\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs)\n",
    "\n",
    "    # Reshape back to original dimensions\n",
    "    return x_rotated.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "\n",
    "class Gemma2RMSNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.norm(2, dim=-1, keepdim=True)\n",
    "        return x * self.weight / (norm + self.eps)\n",
    "\n",
    "\n",
    "class Gemma2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, seq_len: int, theta: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.freqs = precompute_rotary_embedding(dim, seq_len, theta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return apply_rotary_embedding(x, self.freqs[:x.size(1)].to(x.device))\n",
    "\n",
    "\n",
    "class PytorchGELUTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x) * torch.tanh(x)\n",
    "\n",
    "\n",
    "class Gemma2Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, num_heads * head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, num_heads * head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, num_heads * head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(num_heads * head_dim, embed_dim, bias=False)\n",
    "        self.rotary_emb = Gemma2RotaryEmbedding(head_dim, seq_len)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        head_dim = self.q_proj.out_features // self.num_heads\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply rotary embedding\n",
    "        freqs = self.rotary_emb.freqs[:seq_len].to(x.device)\n",
    "        q = apply_rotary_embedding(q, freqs)\n",
    "        k = apply_rotary_embedding(k, freqs)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "\n",
    "        # Apply the causal mask\n",
    "        if mask is not None:\n",
    "            mask = mask.expand(batch_size, self.num_heads, seq_len, seq_len)  # Broadcast to match scores\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        return self.o_proj(attn_output.transpose(1, 2).reshape(batch_size, seq_len, -1))\n",
    "\n",
    "\n",
    "class Gemma2MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(embed_dim, intermediate_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(embed_dim, intermediate_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_dim, embed_dim, bias=False)\n",
    "        self.act_fn = PytorchGELUTanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gated_output = self.gate_proj(x)\n",
    "        up_output = self.up_proj(x)\n",
    "        fused = gated_output * up_output\n",
    "        activated = self.act_fn(fused)\n",
    "        return self.down_proj(activated)\n",
    "\n",
    "\n",
    "class Gemma2DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_dim, intermediate_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.self_attn = Gemma2Attention(embed_dim, num_heads, head_dim, seq_len)\n",
    "        self.mlp = Gemma2MLP(embed_dim, intermediate_dim)\n",
    "        self.input_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x = self.self_attn(x, mask) + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_feedforward_layernorm(x)\n",
    "        x = self.mlp(x) + residual\n",
    "        x = self.post_feedforward_layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Gemma2Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, head_dim, intermediate_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Gemma2DecoderLayer(embed_dim, num_heads, head_dim, intermediate_dim, seq_len) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = Gemma2RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "\n",
    "        # Dynamically generate a causal mask based on the input sequence length\n",
    "        seq_len = input_ids.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, seq_len, seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "    def save_weights(self, directory):\n",
    "        \"\"\"Save weights for each component to a directory.\"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        torch.save(self.embed_tokens.state_dict(), os.path.join(directory, \"embed_tokens.pth\"))\n",
    "        torch.save(self.norm.state_dict(), os.path.join(directory, \"norm.pth\"))\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            torch.save(layer.state_dict(), os.path.join(directory, f\"layer_{i}.pth\"))\n",
    "\n",
    "    def load_weights(self, directory):\n",
    "        \"\"\"Load weights for each component from a directory.\"\"\"\n",
    "        self.embed_tokens.load_state_dict(torch.load(os.path.join(directory, \"embed_tokens.pth\")))\n",
    "        self.norm.load_state_dict(torch.load(os.path.join(directory, \"norm.pth\")))\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.load_state_dict(torch.load(os.path.join(directory, f\"layer_{i}.pth\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example instantiation\n",
    "vocab_size = 256000\n",
    "embed_dim = 2304\n",
    "num_layers = 26\n",
    "num_heads = 8\n",
    "head_dim = 256\n",
    "intermediate_dim = 9216\n",
    "seq_len = 1024\n",
    "\n",
    "model = Gemma2Model(vocab_size, embed_dim, num_layers, num_heads, head_dim, intermediate_dim, seq_len)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 256])\n",
      "Predicted next token: 12\n"
     ]
    }
   ],
   "source": [
    "# Example parameters for a smaller model\n",
    "vocab_size = 25600\n",
    "embed_dim = 256\n",
    "num_layers = 4\n",
    "num_heads = 2\n",
    "head_dim = 128\n",
    "intermediate_dim = 512\n",
    "seq_len = 16\n",
    "\n",
    "# Instantiate smaller model\n",
    "model = Gemma2Model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    head_dim=head_dim,\n",
    "    intermediate_dim=intermediate_dim,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "\n",
    "# Example input\n",
    "example_input = [1, 5, 12]\n",
    "input_tensor = torch.tensor([example_input])  # Shape: [1, seq_len]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_tensor)\n",
    "print(\"Output shape:\", logits.shape)  # Should match [1, seq_len, embed_dim]\n",
    "\n",
    "# Project to vocabulary logits\n",
    "vocab_logits = logits @ model.embed_tokens.weight.T  # Shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Extract logits for the last token\n",
    "last_token_logits = vocab_logits[:, -1, :]  # Shape: [1, vocab_size]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Get the token with the highest probability (greedy decoding)\n",
    "predicted_token = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "print(f\"Predicted next token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
